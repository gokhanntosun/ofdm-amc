{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data from ofdm dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from comm.OFDM import OFDM\n",
    "\n",
    "BASE = '/Users/gtosun/Documents/vsc_workspace/ofdm-amc/data/data_lib/8db'\n",
    "plt.style.use(style='dark_background')\n",
    "\n",
    "xpaths, ypaths = [], [],\n",
    "for root, _, files in os.walk(BASE):\n",
    "    for file in files:\n",
    "        if file == 'x.npy': xpaths.append(os.path.join(root, file))\n",
    "        if file == 'y.npy': ypaths.append(os.path.join(root, file))\n",
    "\n",
    "d = defaultdict(lambda: defaultdict(dict))\n",
    "for xp, yp in zip(sorted(xpaths), sorted(ypaths)):\n",
    "\n",
    "    tmp = xp.split('/')\n",
    "    mod = tmp[9]\n",
    "    m = tmp[10]\n",
    "    size = tmp[11]\n",
    "\n",
    "    d[f'{m}{mod}'][size]['x'] = np.load(xp)\n",
    "    d[f'{m}{mod}'][size]['y'] = np.load(yp)\n",
    "\n",
    "frame = d['8psk']['1024point']['x'][:, :, 998]\n",
    "windows = [256, 512, 1024]\n",
    "\n",
    "filter_bank = [OFDM(n_carriers=n) for n in windows]\n",
    "t_signals = [filter.demodulate(frame) for filter in filter_bank]\n",
    "fig, axs  = plt.subplots(len(t_signals), 1)\n",
    "\n",
    "for ax, t in zip(axs, t_signals):\n",
    "    ax.step(range(len(t[0,:])), t[0,:])\n",
    "    ax.set_xlim(left=0, right=128)\n",
    "    ax.set_ylim(bottom=-3, top=3)\n",
    "    ax.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot constellations from dataset \n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "BASE = '/Users/gtosun/Documents/vsc_workspace/modulation-classification/data/categorical/8db'\n",
    "plt.style.use(style='dark_background')\n",
    "\n",
    "xpaths, ypaths = [], [],\n",
    "for root, _, files in os.walk(BASE):\n",
    "    for file in files:\n",
    "        if file == 'x.npy': xpaths.append(os.path.join(root, file))\n",
    "        if file == 'y.npy': ypaths.append(os.path.join(root, file))\n",
    "\n",
    "d = defaultdict(lambda: dict())\n",
    "for xp, yp in zip(sorted(xpaths), sorted(ypaths)):\n",
    "\n",
    "    tmp = xp.split('/')\n",
    "    m = tmp[-2]\n",
    "    mod = tmp[-3]\n",
    "\n",
    "    d[f'{m}{mod}']['x'] = np.load(xp)\n",
    "    d[f'{m}{mod}']['y'] = np.load(yp)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(style='dark_background')\n",
    "mods = [\"2psk\", \"4psk\", \"4qam\", \"8psk\", \"16qam\", \"16psk\", \"64qam\"]\n",
    "label_dict = {i: mod for i, mod in enumerate(mods)}\n",
    "\n",
    "ks = list(d.keys())\n",
    "num_types = len(ks)\n",
    "\n",
    "nrows = 2\n",
    "ncols = 4\n",
    "\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols)\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "\n",
    "        idx = i * ncols + j\n",
    "        if idx >= num_types:\n",
    "            fig.delaxes(axs[i][j])\n",
    "            break\n",
    "\n",
    "        k = ks[idx]\n",
    "\n",
    "        x = d[k]['x'][0, :, :]\n",
    "        xi, xq = x[0, :], x[1, :]\n",
    "\n",
    "        y = d[k]['y'][0]\n",
    "        _, idx = torch.max(torch.squeeze(torch.Tensor(d[k]['y'][0])), 0)\n",
    "    \n",
    "        axs[i][j].set_title(label_dict[idx.item()])\n",
    "        axs[i][j].scatter(xi, xq, marker='s', s=0.75, c='c')\n",
    "        axs[i][j].set_xlim(left=-1.5, right=1.5)\n",
    "        axs[i][j].set_ylim(bottom=-1.5, top=1.5)\n",
    "        axs[i][j].set_aspect('equal')\n",
    "        axs[i][j].margins(0)\n",
    "        axs[i][j].grid(True)\n",
    "\n",
    "fig.set_size_inches(w=ncols * 2, h=nrows * 2)\n",
    "fig.suptitle('constellations')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try torch.stft and torch.istft here \n",
    "import torch\n",
    "\n",
    "def transform(x, n: int):\n",
    "    tmp = torch.stft(input=x, n_fft=n, hop_length=n, center=True, normalized=True, onesided=False, return_complex=True)\n",
    "    tmp = torch.reshape(tmp, (1, tmp.numel()))\n",
    "    return torch.row_stack((torch.real(tmp), torch.imag(tmp)))\n",
    "\n",
    "complex_frame = torch.from_numpy( frame[0, :] + 1j * frame[1, :] )\n",
    "u = [transform(complex_frame, n) for n in windows]\n",
    "\n",
    "fig, axs  = plt.subplots(len(u), 1)\n",
    "for ax, t in zip(axs, u):\n",
    "    ax.step(range(len(t[1,:])), t[1,:])\n",
    "    ax.set_xlim(left=0, right=64)\n",
    "    ax.set_ylim(bottom=-3, top=3)\n",
    "    ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gtosun/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/7650], Loss: 1.1784\n",
      "Epoch [1/3], Step [200/7650], Loss: 1.0714\n",
      "Epoch [1/3], Step [300/7650], Loss: 1.1241\n",
      "Epoch [1/3], Step [400/7650], Loss: 1.1133\n",
      "Epoch [1/3], Step [500/7650], Loss: 1.1065\n",
      "Epoch [1/3], Step [600/7650], Loss: 1.1088\n",
      "Epoch [1/3], Step [700/7650], Loss: 1.0995\n",
      "Epoch [1/3], Step [800/7650], Loss: 1.1095\n",
      "Epoch [1/3], Step [900/7650], Loss: 1.0764\n",
      "Epoch [1/3], Step [1000/7650], Loss: 1.1015\n",
      "Epoch [1/3], Step [1100/7650], Loss: 1.0926\n",
      "Epoch [1/3], Step [1200/7650], Loss: 1.0948\n",
      "Epoch [1/3], Step [1300/7650], Loss: 1.0948\n",
      "Epoch [1/3], Step [1400/7650], Loss: 1.0880\n",
      "Epoch [1/3], Step [1500/7650], Loss: 1.1006\n",
      "Epoch [1/3], Step [1600/7650], Loss: 1.0871\n",
      "Epoch [1/3], Step [1700/7650], Loss: 1.0928\n",
      "Epoch [1/3], Step [1800/7650], Loss: 1.0813\n",
      "Epoch [1/3], Step [1900/7650], Loss: 1.0716\n",
      "Epoch [1/3], Step [2000/7650], Loss: 1.0694\n",
      "Epoch [1/3], Step [2100/7650], Loss: 1.0526\n",
      "Epoch [1/3], Step [2200/7650], Loss: 1.0682\n",
      "Epoch [1/3], Step [2300/7650], Loss: 1.0606\n",
      "Epoch [1/3], Step [2400/7650], Loss: 1.0250\n",
      "Epoch [1/3], Step [2500/7650], Loss: 1.0167\n",
      "Epoch [1/3], Step [2600/7650], Loss: 1.0342\n",
      "Epoch [1/3], Step [2700/7650], Loss: 1.0102\n",
      "Epoch [1/3], Step [2800/7650], Loss: 1.0092\n",
      "Epoch [1/3], Step [2900/7650], Loss: 0.9314\n",
      "Epoch [1/3], Step [3000/7650], Loss: 0.9083\n",
      "Epoch [1/3], Step [3100/7650], Loss: 0.9913\n",
      "Epoch [1/3], Step [3200/7650], Loss: 0.9006\n",
      "Epoch [1/3], Step [3300/7650], Loss: 0.8775\n",
      "Epoch [1/3], Step [3400/7650], Loss: 0.8578\n",
      "Epoch [1/3], Step [3500/7650], Loss: 0.8448\n",
      "Epoch [1/3], Step [3600/7650], Loss: 0.7554\n",
      "Epoch [1/3], Step [3700/7650], Loss: 0.6858\n",
      "Epoch [1/3], Step [3800/7650], Loss: 0.7920\n",
      "Epoch [1/3], Step [3900/7650], Loss: 0.5803\n",
      "Epoch [1/3], Step [4000/7650], Loss: 0.4994\n",
      "Epoch [1/3], Step [4100/7650], Loss: 0.5992\n",
      "Epoch [1/3], Step [4200/7650], Loss: 0.4895\n",
      "Epoch [1/3], Step [4300/7650], Loss: 0.6434\n",
      "Epoch [1/3], Step [4400/7650], Loss: 0.6053\n",
      "Epoch [1/3], Step [4500/7650], Loss: 0.5988\n",
      "Epoch [1/3], Step [4600/7650], Loss: 0.5292\n",
      "Epoch [1/3], Step [4700/7650], Loss: 0.4811\n",
      "Epoch [1/3], Step [4800/7650], Loss: 0.4983\n",
      "Epoch [1/3], Step [4900/7650], Loss: 0.5542\n",
      "Epoch [1/3], Step [5000/7650], Loss: 0.4766\n",
      "Epoch [1/3], Step [5100/7650], Loss: 0.6431\n",
      "Epoch [1/3], Step [5200/7650], Loss: 0.4169\n",
      "Epoch [1/3], Step [5300/7650], Loss: 0.4699\n",
      "Epoch [1/3], Step [5400/7650], Loss: 0.4288\n",
      "Epoch [1/3], Step [5500/7650], Loss: 0.3732\n",
      "Epoch [1/3], Step [5600/7650], Loss: 0.3918\n",
      "Epoch [1/3], Step [5700/7650], Loss: 0.4406\n",
      "Epoch [1/3], Step [5800/7650], Loss: 0.3544\n",
      "Epoch [1/3], Step [5900/7650], Loss: 0.3941\n",
      "Epoch [1/3], Step [6000/7650], Loss: 0.3480\n",
      "Epoch [1/3], Step [6100/7650], Loss: 0.3916\n",
      "Epoch [1/3], Step [6200/7650], Loss: 0.4374\n",
      "Epoch [1/3], Step [6300/7650], Loss: 0.3302\n",
      "Epoch [1/3], Step [6400/7650], Loss: 0.2657\n",
      "Epoch [1/3], Step [6500/7650], Loss: 0.2990\n",
      "Epoch [1/3], Step [6600/7650], Loss: 0.2828\n",
      "Epoch [1/3], Step [6700/7650], Loss: 0.3703\n",
      "Epoch [1/3], Step [6800/7650], Loss: 0.4005\n",
      "Epoch [1/3], Step [6900/7650], Loss: 0.3263\n",
      "Epoch [1/3], Step [7000/7650], Loss: 0.4390\n",
      "Epoch [1/3], Step [7100/7650], Loss: 0.4966\n",
      "Epoch [1/3], Step [7200/7650], Loss: 0.3028\n",
      "Epoch [1/3], Step [7300/7650], Loss: 0.3651\n",
      "Epoch [1/3], Step [7400/7650], Loss: 0.2487\n",
      "Epoch [1/3], Step [7500/7650], Loss: 0.1893\n",
      "Epoch [1/3], Step [7600/7650], Loss: 0.2309\n",
      "Accuracy of the network on the test set: 89.92592592592592 %\n",
      "Epoch [2/3], Step [100/7650], Loss: 0.1545\n",
      "Epoch [2/3], Step [200/7650], Loss: 0.1912\n",
      "Epoch [2/3], Step [300/7650], Loss: 0.2687\n",
      "Epoch [2/3], Step [400/7650], Loss: 0.1369\n",
      "Epoch [2/3], Step [500/7650], Loss: 0.1291\n",
      "Epoch [2/3], Step [600/7650], Loss: 0.2852\n",
      "Epoch [2/3], Step [700/7650], Loss: 0.1600\n",
      "Epoch [2/3], Step [800/7650], Loss: 0.2167\n",
      "Epoch [2/3], Step [900/7650], Loss: 0.1510\n",
      "Epoch [2/3], Step [1000/7650], Loss: 0.1568\n",
      "Epoch [2/3], Step [1100/7650], Loss: 0.1474\n",
      "Epoch [2/3], Step [1200/7650], Loss: 0.1070\n",
      "Epoch [2/3], Step [1300/7650], Loss: 0.1822\n",
      "Epoch [2/3], Step [1400/7650], Loss: 0.1090\n",
      "Epoch [2/3], Step [1500/7650], Loss: 0.2463\n",
      "Epoch [2/3], Step [1600/7650], Loss: 0.2621\n",
      "Epoch [2/3], Step [1700/7650], Loss: 0.0937\n",
      "Epoch [2/3], Step [1800/7650], Loss: 0.4097\n",
      "Epoch [2/3], Step [1900/7650], Loss: 0.1402\n",
      "Epoch [2/3], Step [2000/7650], Loss: 0.1515\n",
      "Epoch [2/3], Step [2100/7650], Loss: 0.1007\n",
      "Epoch [2/3], Step [2200/7650], Loss: 0.2690\n",
      "Epoch [2/3], Step [2300/7650], Loss: 0.1332\n",
      "Epoch [2/3], Step [2400/7650], Loss: 0.1525\n",
      "Epoch [2/3], Step [2500/7650], Loss: 0.0743\n",
      "Epoch [2/3], Step [2600/7650], Loss: 0.1367\n",
      "Epoch [2/3], Step [2700/7650], Loss: 0.1805\n",
      "Epoch [2/3], Step [2800/7650], Loss: 0.1357\n",
      "Epoch [2/3], Step [2900/7650], Loss: 0.2501\n",
      "Epoch [2/3], Step [3000/7650], Loss: 0.1931\n",
      "Epoch [2/3], Step [3100/7650], Loss: 0.1284\n",
      "Epoch [2/3], Step [3200/7650], Loss: 0.0614\n",
      "Epoch [2/3], Step [3300/7650], Loss: 0.0896\n",
      "Epoch [2/3], Step [3400/7650], Loss: 0.0856\n",
      "Epoch [2/3], Step [3500/7650], Loss: 0.2359\n",
      "Epoch [2/3], Step [3600/7650], Loss: 0.1043\n",
      "Epoch [2/3], Step [3700/7650], Loss: 0.1091\n",
      "Epoch [2/3], Step [3800/7650], Loss: 0.1486\n",
      "Epoch [2/3], Step [3900/7650], Loss: 0.3932\n",
      "Epoch [2/3], Step [4000/7650], Loss: 0.0853\n",
      "Epoch [2/3], Step [4100/7650], Loss: 0.2125\n",
      "Epoch [2/3], Step [4200/7650], Loss: 0.0765\n",
      "Epoch [2/3], Step [4300/7650], Loss: 0.1565\n",
      "Epoch [2/3], Step [4400/7650], Loss: 0.2148\n",
      "Epoch [2/3], Step [4500/7650], Loss: 0.1889\n",
      "Epoch [2/3], Step [4600/7650], Loss: 0.1855\n",
      "Epoch [2/3], Step [4700/7650], Loss: 0.1656\n",
      "Epoch [2/3], Step [4800/7650], Loss: 0.1412\n",
      "Epoch [2/3], Step [4900/7650], Loss: 0.1450\n",
      "Epoch [2/3], Step [5000/7650], Loss: 0.1678\n",
      "Epoch [2/3], Step [5100/7650], Loss: 0.1254\n",
      "Epoch [2/3], Step [5200/7650], Loss: 0.0560\n",
      "Epoch [2/3], Step [5300/7650], Loss: 0.1434\n",
      "Epoch [2/3], Step [5400/7650], Loss: 0.2747\n",
      "Epoch [2/3], Step [5500/7650], Loss: 0.1147\n",
      "Epoch [2/3], Step [5600/7650], Loss: 0.1384\n",
      "Epoch [2/3], Step [5700/7650], Loss: 0.2289\n",
      "Epoch [2/3], Step [5800/7650], Loss: 0.1152\n",
      "Epoch [2/3], Step [5900/7650], Loss: 0.1381\n",
      "Epoch [2/3], Step [6000/7650], Loss: 0.2745\n",
      "Epoch [2/3], Step [6100/7650], Loss: 0.1377\n",
      "Epoch [2/3], Step [6200/7650], Loss: 0.1383\n",
      "Epoch [2/3], Step [6300/7650], Loss: 0.2272\n",
      "Epoch [2/3], Step [6400/7650], Loss: 0.4645\n",
      "Epoch [2/3], Step [6500/7650], Loss: 0.0939\n",
      "Epoch [2/3], Step [6600/7650], Loss: 0.1065\n",
      "Epoch [2/3], Step [6700/7650], Loss: 0.1841\n",
      "Epoch [2/3], Step [6800/7650], Loss: 0.0739\n",
      "Epoch [2/3], Step [6900/7650], Loss: 0.0597\n",
      "Epoch [2/3], Step [7000/7650], Loss: 0.2249\n",
      "Epoch [2/3], Step [7100/7650], Loss: 0.0810\n",
      "Epoch [2/3], Step [7200/7650], Loss: 0.1051\n",
      "Epoch [2/3], Step [7300/7650], Loss: 0.0441\n",
      "Epoch [2/3], Step [7400/7650], Loss: 0.0953\n",
      "Epoch [2/3], Step [7500/7650], Loss: 0.1383\n",
      "Epoch [2/3], Step [7600/7650], Loss: 0.3291\n",
      "Accuracy of the network on the test set: 96.0 %\n",
      "Epoch [3/3], Step [100/7650], Loss: 0.0500\n",
      "Epoch [3/3], Step [200/7650], Loss: 0.0203\n",
      "Epoch [3/3], Step [300/7650], Loss: 0.1122\n",
      "Epoch [3/3], Step [400/7650], Loss: 0.0481\n",
      "Epoch [3/3], Step [500/7650], Loss: 0.0230\n",
      "Epoch [3/3], Step [600/7650], Loss: 0.1409\n",
      "Epoch [3/3], Step [700/7650], Loss: 0.0449\n",
      "Epoch [3/3], Step [800/7650], Loss: 0.0192\n",
      "Epoch [3/3], Step [900/7650], Loss: 0.0250\n",
      "Epoch [3/3], Step [1000/7650], Loss: 0.0406\n",
      "Epoch [3/3], Step [1100/7650], Loss: 0.1241\n",
      "Epoch [3/3], Step [1200/7650], Loss: 0.0330\n",
      "Epoch [3/3], Step [1300/7650], Loss: 0.0838\n",
      "Epoch [3/3], Step [1400/7650], Loss: 0.0596\n",
      "Epoch [3/3], Step [1500/7650], Loss: 0.0263\n",
      "Epoch [3/3], Step [1600/7650], Loss: 0.0597\n",
      "Epoch [3/3], Step [1700/7650], Loss: 0.1036\n",
      "Epoch [3/3], Step [1800/7650], Loss: 0.0365\n",
      "Epoch [3/3], Step [1900/7650], Loss: 0.0140\n",
      "Epoch [3/3], Step [2000/7650], Loss: 0.0234\n",
      "Epoch [3/3], Step [2100/7650], Loss: 0.0317\n",
      "Epoch [3/3], Step [2200/7650], Loss: 0.0415\n",
      "Epoch [3/3], Step [2300/7650], Loss: 0.0775\n",
      "Epoch [3/3], Step [2400/7650], Loss: 0.2986\n",
      "Epoch [3/3], Step [2500/7650], Loss: 0.1677\n",
      "Epoch [3/3], Step [2600/7650], Loss: 0.0388\n",
      "Epoch [3/3], Step [2700/7650], Loss: 0.1034\n",
      "Epoch [3/3], Step [2800/7650], Loss: 0.0259\n",
      "Epoch [3/3], Step [2900/7650], Loss: 0.0278\n",
      "Epoch [3/3], Step [3000/7650], Loss: 0.0539\n",
      "Epoch [3/3], Step [3100/7650], Loss: 0.2685\n",
      "Epoch [3/3], Step [3200/7650], Loss: 0.2432\n",
      "Epoch [3/3], Step [3300/7650], Loss: 0.1896\n",
      "Epoch [3/3], Step [3400/7650], Loss: 0.0385\n",
      "Epoch [3/3], Step [3500/7650], Loss: 0.0570\n",
      "Epoch [3/3], Step [3600/7650], Loss: 0.0865\n",
      "Epoch [3/3], Step [3700/7650], Loss: 0.1989\n",
      "Epoch [3/3], Step [3800/7650], Loss: 0.0960\n",
      "Epoch [3/3], Step [3900/7650], Loss: 0.2698\n",
      "Epoch [3/3], Step [4000/7650], Loss: 0.1101\n",
      "Epoch [3/3], Step [4100/7650], Loss: 0.0828\n",
      "Epoch [3/3], Step [4200/7650], Loss: 0.0379\n",
      "Epoch [3/3], Step [4300/7650], Loss: 0.0657\n",
      "Epoch [3/3], Step [4400/7650], Loss: 0.1180\n",
      "Epoch [3/3], Step [4500/7650], Loss: 0.0587\n",
      "Epoch [3/3], Step [4600/7650], Loss: 0.1102\n",
      "Epoch [3/3], Step [4700/7650], Loss: 0.1983\n",
      "Epoch [3/3], Step [4800/7650], Loss: 0.0536\n",
      "Epoch [3/3], Step [4900/7650], Loss: 0.0163\n",
      "Epoch [3/3], Step [5000/7650], Loss: 0.0714\n",
      "Epoch [3/3], Step [5100/7650], Loss: 0.0540\n",
      "Epoch [3/3], Step [5200/7650], Loss: 0.0417\n",
      "Epoch [3/3], Step [5300/7650], Loss: 0.0215\n",
      "Epoch [3/3], Step [5400/7650], Loss: 0.0533\n",
      "Epoch [3/3], Step [5500/7650], Loss: 0.0497\n",
      "Epoch [3/3], Step [5600/7650], Loss: 0.0298\n",
      "Epoch [3/3], Step [5700/7650], Loss: 0.0290\n",
      "Epoch [3/3], Step [5800/7650], Loss: 0.2797\n",
      "Epoch [3/3], Step [5900/7650], Loss: 0.1323\n",
      "Epoch [3/3], Step [6000/7650], Loss: 0.0245\n",
      "Epoch [3/3], Step [6100/7650], Loss: 0.0795\n",
      "Epoch [3/3], Step [6200/7650], Loss: 0.1607\n",
      "Epoch [3/3], Step [6300/7650], Loss: 0.0389\n",
      "Epoch [3/3], Step [6400/7650], Loss: 0.0150\n",
      "Epoch [3/3], Step [6500/7650], Loss: 0.0199\n",
      "Epoch [3/3], Step [6600/7650], Loss: 0.0464\n",
      "Epoch [3/3], Step [6700/7650], Loss: 0.3231\n",
      "Epoch [3/3], Step [6800/7650], Loss: 0.1946\n",
      "Epoch [3/3], Step [6900/7650], Loss: 0.0137\n",
      "Epoch [3/3], Step [7000/7650], Loss: 0.1848\n",
      "Epoch [3/3], Step [7100/7650], Loss: 0.1005\n",
      "Epoch [3/3], Step [7200/7650], Loss: 0.0974\n",
      "Epoch [3/3], Step [7300/7650], Loss: 0.2024\n",
      "Epoch [3/3], Step [7400/7650], Loss: 0.1097\n",
      "Epoch [3/3], Step [7500/7650], Loss: 0.1819\n",
      "Epoch [3/3], Step [7600/7650], Loss: 0.0909\n",
      "Accuracy of the network on the test set: 89.77777777777777 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from model.AMCModel import AMCModel\n",
    "from data.dataset.OFDMDataset import OFDMDataset, get_dataloaders\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "\n",
    "LR = 0.0001\n",
    "BATCH_SIZE = 1\n",
    "SHUFFLE = True\n",
    "EPOCHS = 3\n",
    "\n",
    "dataset = OFDMDataset()\n",
    "trainloader, testloader = get_dataloaders(dataset=dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE)\n",
    "\n",
    "model = AMCModel()\n",
    "model = model.train()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(params=model.parameters(), lr=LR)\n",
    "\n",
    "n_total_steps = len(trainloader)\n",
    "losses = []\n",
    "predictions = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model = model.train()\n",
    "    batch_losses = list()\n",
    "    for i, (x, y) in enumerate(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        scores = model(x)\n",
    "        loss = criterion(scores, torch.squeeze(y))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.item())\n",
    "        predictions.append(torch.argmax(scores).item())\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{EPOCHS}], Step [{i+1}/{n_total_steps}], Loss: {np.mean(batch_losses):.4f}')\n",
    "            losses.append(np.mean(batch_losses))\n",
    "            batch_losses.clear()\n",
    "\n",
    "    model = model.eval()\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for x, y in testloader: \n",
    "            scores = model.forward(x)\n",
    "            predicted = torch.argmax(scores).item()\n",
    "            labels = torch.argmax(torch.squeeze(y)).item()\n",
    "            n_samples += y.size(0)\n",
    "            n_correct += predicted == labels\n",
    "\n",
    "        acc = 100.0 * n_correct / n_samples\n",
    "        print(f'Accuracy of the network on the test set: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "def impl(n: int, x: torch.tensor):\n",
    "    x = x[:, 0, :] + 1j * x[:, 1, :]\n",
    "    y = torch.stft(input=x, n_fft=n, hop_length=n, center=False, normalized=True, onesided=False, return_complex=False).movedim(-1, 1).flatten(2, 3)\n",
    "    return y\n",
    "\n",
    "windows = [256, 512, 1024]\n",
    "res = [impl(w, frame) for w in windows]\n",
    "\n",
    "print(res[0].shape)\n",
    "\n",
    "fig, axs  = plt.subplots(len(res), 1)\n",
    "for ax, t in zip(axs, res):\n",
    "    ax.step(range(len(t[0,0,:])), t[0,1,:])\n",
    "    ax.set_xlim(left=0, right=64)\n",
    "    ax.set_ylim(bottom=-3, top=3)\n",
    "    ax.grid(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
